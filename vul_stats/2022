12172
[('CWE-79', 2848), ('CWE-787', 1679), ('CWE-89', 1606), ('CWE-416', 631), ('CWE-125', 604), ('CWE-22', 578), ('CWE-352', 556), ('CWE-20', 470), ('NVD-CWE-Other', 460), ('CWE-863', 442), ('CWE-269', 410), ('CWE-434', 408), ('CWE-77', 386), ('CWE-862', 378), ('CWE-287', 362), ('CWE-120', 354), ('CWE-400', 283), ('CWE-668', 282), ('CWE-78', 275), ('CWE-476', 237), ('CWE-200', 224), ('CWE-918', 185), ('CWE-190', 182), ('CWE-502', 178), ('CWE-119', 173), ('CWE-94', 169), ('CWE-798', 165), ('CWE-276', 155), ('CWE-362', 153), ('CWE-770', 142), ('CWE-732', 130), ('CWE-601', 121), ('CWE-522', 117), ('CWE-74', 112), ('CWE-639', 106), ('CWE-401', 100), ('CWE-611', 96), ('CWE-306', 95), ('CWE-617', 91), ('CWE-427', 89), ('CWE-532', 86), ('CWE-295', 81), ('CWE-284', 70), ('CWE-755', 70), ('CWE-552', 68), ('CWE-707', 65), ('CWE-203', 65), ('CWE-312', 64), ('CWE-122', 63), ('CWE-1321', 61), ('CWE-59', 60), ('CWE-121', 59), ('CWE-367', 56), ('CWE-754', 52), ('CWE-307', 52), ('CWE-319', 51), ('CWE-613', 47), ('CWE-327', 46), ('CWE-835', 45), ('CWE-345', 45), ('CWE-347', 42), ('CWE-1021', 41), ('CWE-1236', 40), ('CWE-415', 40), ('CWE-843', 40), ('CWE-326', 39), ('CWE-521', 37), ('CWE-290', 35), ('CWE-824', 35), ('CWE-384', 33), ('CWE-294', 32), ('CWE-665', 32), ('CWE-209', 32), ('CWE-404', 32), ('CWE-330', 32), ('CWE-610', 30), ('CWE-444', 30), ('CWE-428', 30), ('CWE-88', 29), ('CWE-134', 27), ('CWE-674', 26), ('CWE-281', 24), ('CWE-908', 24), ('CWE-346', 23), ('CWE-667', 20), ('CWE-311', 20), ('CWE-252', 20), ('CWE-682', 20), ('CWE-426', 19), ('CWE-129', 19), ('CWE-697', 18), ('CWE-116', 18), ('CWE-772', 18), ('CWE-494', 18), ('CWE-191', 18), ('CWE-354', 18), ('CWE-922', 18), ('CWE-285', 17), ('CWE-369', 17), ('CWE-459', 16), ('CWE-829', 16), ('CWE-256', 15), ('CWE-640', 15), ('CWE-131', 15), ('CWE-91', 13), ('CWE-1188', 13), ('CWE-693', 13), ('CWE-338', 12), ('CWE-662', 11), ('CWE-425', 11), ('CWE-704', 11), ('CWE-670', 11), ('CWE-763', 10), ('CWE-1333', 10), ('CWE-436', 9), ('CWE-706', 9), ('CWE-126', 9), ('CWE-916', 9), ('CWE-193', 8), ('CWE-681', 8), ('CWE-264', 8), ('CWE-669', 7), ('CWE-23', 7), ('CWE-909', 7), ('CWE-776', 7), ('CWE-913', 7), ('CWE-266', 7), ('CWE-565', 6), ('CWE-1220', 6), ('CWE-672', 6), ('CWE-359', 6), ('CWE-250', 5), ('CWE-322', 5), ('CWE-212', 5), ('CWE-377', 5), ('CWE-80', 5), ('CWE-823', 5), ('CWE-288', 4), ('CWE-840', 4), ('CWE-331', 4), ('CWE-93', 4), ('CWE-940', 4), ('CWE-822', 4), ('CWE-204', 3), ('CWE-789', 3), ('CWE-305', 3), ('CWE-73', 3), ('CWE-61', 3), ('CWE-648', 3), ('CWE-248', 3), ('CWE-917', 3), ('CWE-321', 2), ('CWE-318', 2), ('CWE-1284', 2), ('CWE-453', 2), ('CWE-241', 2), ('CWE-328', 2), ('CWE-489', 2), ('CWE-1274', 2), ('CWE-117', 2), ('CWE-549', 2), ('CWE-834', 2), ('CWE-1258', 2), ('CWE-95', 2), ('CWE-99', 2), ('CWE-620', 2), ('CWE-335', 2), ('CWE-680', 2), ('CWE-297', 2), ('CWE-1336', 2), ('CWE-440', 2), ('CWE-838', 2), ('CWE-75', 1), ('CWE-524', 1), ('CWE-791', 1), ('CWE-302', 1), ('CWE-124', 1), ('CWE-455', 1), ('CWE-833', 1), ('CWE-21', 1), ('CWE-208', 1), ('CWE-1320', 1), ('CWE-146', 1), ('CWE-1287', 1), ('CWE-911', 1), ('CWE-1108', 1), ('CWE-130', 1), ('CWE-324', 1), ('CWE-304', 1), ('CWE-273', 1), ('CWE-279', 1), ('CWE-923', 1), ('CWE-1022', 1), ('CWE-303', 1), ('CWE-29', 1), ('CWE-523', 1), ('CWE-391', 1), ('CWE-1288', 1), ('CWE-84', 1), ('CWE-98', 1), ('CWE-1390', 1), ('CWE-470', 1), ('CWE-912', 1), ('CWE-451', 1), ('CWE-27', 1), ('CWE-805', 1), ('CWE-24', 1), ('CWE-379', 1), ('CWE-779', 1), ('CWE-603', 1), ('CWE-671', 1), ('CWE-280', 1), ('CWE-187', 1), ('CWE-475', 1), ('CWE-690', 1), ('CWE-201', 1), ('CWE-924', 1), ('CWE-602', 1), ('CWE-1283', 1), ('CWE-115', 1), ('CWE-141', 1), ('CWE-460', 1), ('CWE-274', 1), ('CWE-409', 1), ('CWE-300', 1), ('CWE-1282', 1), ('CWE-424', 1), ('CWE-229', 1), ('CWE-548', 1), ('CWE-282', 1), ('CWE-1026', 1), ('CWE-194', 1), ('CWE-112', 1), ('CWE-941', 1), ('CWE-178', 1)]
235
{'CWE-434', 'CWE-416', 'CWE-89', 'CWE-120', 'CWE-22', 'NVD-CWE-Other', 'CWE-862', 'CWE-125', 'CWE-787', 'CWE-352', 'CWE-287', 'CWE-269', 'CWE-77', 'CWE-79', 'CWE-20', 'CWE-863'}
16
12172
16

<wv------------------------------------------------------------------------------------------->
year = '2022'
vec_len = 100
min_count = 1
window_len = 5
dense_unit = 128
wv_model_path  = '..//..//models//wv//'+year+"_"+str(vec_len)+"_"+str(min_count)+"_"+str(window_len)+'.pkl'
label_path = '..\\..\\data\\clean\\nvdcve-1.1-'+year+'_labels.csv'
n=30
cwe_count = 16  # 2022

self.c1 = Conv2D(filters=12, kernel_size=(3, vec_len), padding='same')  # 卷积层
        self.b1 = BatchNormalization()  # BN层
        self.a1 = Activation('relu')  # 激活层
        self.p1 = MaxPool2D(pool_size=(2, 2), strides=2, padding='same')  # 池化层
        self.d1 = Dropout(0.2)  # dropout层

        self.flatten = Flatten()
        self.f1 = Dense(dense_unit, activation='relu')
        self.d2 = Dropout(0.2)
        self.f2 = Dense(cwe_count, activation='softmax')


305/305 [==============================] - 3s 3ms/step - loss: 2.3942 - accuracy: 0.3961
Epoch 2/10
305/305 [==============================] - 1s 3ms/step - loss: 1.3425 - accuracy: 0.5678
Epoch 3/10
305/305 [==============================] - 1s 3ms/step - loss: 1.1714 - accuracy: 0.6086
Epoch 4/10
305/305 [==============================] - 1s 3ms/step - loss: 1.0566 - accuracy: 0.6537
Epoch 5/10
305/305 [==============================] - 1s 3ms/step - loss: 0.9855 - accuracy: 0.6683
Epoch 6/10
305/305 [==============================] - 1s 3ms/step - loss: 0.9001 - accuracy: 0.6884
Epoch 7/10
305/305 [==============================] - 1s 3ms/step - loss: 0.8775 - accuracy: 0.6992
Epoch 8/10
305/305 [==============================] - 1s 3ms/step - loss: 0.8268 - accuracy: 0.7171
Epoch 9/10
305/305 [==============================] - 1s 3ms/step - loss: 0.7709 - accuracy: 0.7354
Epoch 10/10
305/305 [==============================] - 1s 3ms/step - loss: 0.7404 - accuracy: 0.7453
77/77 [==============================] - 0s 2ms/step - loss: 0.8299 - accuracy: 0.7462
last score: [0.8299335241317749, 0.74620121717453]
</wv------------------------------------------------------------------------------------------->

<tfi-wv---------------------------------------------------------------------------------------->
year = '2022'
vec_len = 100
min_count = 1
window_len = 5
ig_num = 1000
sentence_len = 30
n=sentence_len
dense_unit = 128
cwe_count = 16  # 2022

305/305 [==============================] - 3s 3ms/step - loss: 2.4681 - accuracy: 0.3982
Epoch 2/10
305/305 [==============================] - 1s 3ms/step - loss: 1.2794 - accuracy: 0.5851
Epoch 3/10
305/305 [==============================] - 1s 3ms/step - loss: 1.1427 - accuracy: 0.6185
Epoch 4/10
305/305 [==============================] - 1s 3ms/step - loss: 1.0425 - accuracy: 0.6521
Epoch 5/10
305/305 [==============================] - 1s 3ms/step - loss: 0.9565 - accuracy: 0.6756
Epoch 6/10
305/305 [==============================] - 1s 3ms/step - loss: 0.9016 - accuracy: 0.6925
Epoch 7/10
305/305 [==============================] - 1s 3ms/step - loss: 0.8608 - accuracy: 0.7107
Epoch 8/10
305/305 [==============================] - 1s 3ms/step - loss: 0.8408 - accuracy: 0.7093
Epoch 9/10
305/305 [==============================] - 1s 3ms/step - loss: 0.7797 - accuracy: 0.7373
Epoch 10/10
305/305 [==============================] - 1s 3ms/step - loss: 0.7590 - accuracy: 0.7351
77/77 [==============================] - 0s 2ms/step - loss: 0.8280 - accuracy: 0.7454
last score: [0.8280287384986877, 0.7453798651695251]
</tfi-wv----------------------------------------------------------------------------------------->

<fast------------------------------------------------------------------------------------------->
year = '2022'
vec_len = 100
min_count = 1
window_len = 5
dense_unit = 128
fast_model_path  = '..//..//models//fasttext//'+year+"_"+str(vec_len)+"_"+str(min_count)+"_"+str(window_len)+'.pkl'
label_path = '..\\..\\data\\clean\\nvdcve-1.1-'+year+'_labels.csv'
n=30
cwe_count = 16  # 2022

self.c1 = Conv2D(filters=12, kernel_size=(3, vec_len), padding='same')  # 卷积层
        self.b1 = BatchNormalization()  # BN层
        self.a1 = Activation('relu')  # 激活层
        self.p1 = MaxPool2D(pool_size=(2, 2), strides=2, padding='same')  # 池化层
        self.d1 = Dropout(0.2)  # dropout层

        self.flatten = Flatten()
        self.f1 = Dense(dense_unit, activation='relu')
        self.d2 = Dropout(0.2)
        self.f2 = Dense(cwe_count, activation='softmax')

305/305 [==============================] - 3s 3ms/step - loss: 2.7078 - accuracy: 0.3450
Epoch 2/10
305/305 [==============================] - 1s 3ms/step - loss: 1.4142 - accuracy: 0.5474
Epoch 3/10
305/305 [==============================] - 1s 3ms/step - loss: 1.2329 - accuracy: 0.6018
Epoch 4/10
305/305 [==============================] - 1s 3ms/step - loss: 1.1344 - accuracy: 0.6240
Epoch 5/10
305/305 [==============================] - 1s 3ms/step - loss: 1.0403 - accuracy: 0.6477
Epoch 6/10
305/305 [==============================] - 1s 3ms/step - loss: 0.9764 - accuracy: 0.6748
Epoch 7/10
305/305 [==============================] - 1s 3ms/step - loss: 0.9507 - accuracy: 0.6747
Epoch 8/10
305/305 [==============================] - 1s 3ms/step - loss: 0.9196 - accuracy: 0.6838
Epoch 9/10
305/305 [==============================] - 1s 3ms/step - loss: 0.8871 - accuracy: 0.6932
Epoch 10/10
305/305 [==============================] - 1s 3ms/step - loss: 0.8452 - accuracy: 0.7079
77/77 [==============================] - 0s 2ms/step - loss: 0.8662 - accuracy: 0.7285
last score: [0.866229772567749, 0.7285420894622803]
</fast------------------------------------------------------------------------------------------->
