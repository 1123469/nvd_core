8100
[('CWE-79', 2347), ('CWE-787', 1296), ('CWE-89', 704), ('CWE-125', 694), ('CWE-20', 548), ('CWE-22', 474), ('CWE-78', 473), ('CWE-352', 434), ('NVD-CWE-Other', 429), ('CWE-416', 379), ('CWE-120', 322), ('CWE-276', 288), ('CWE-862', 275), ('CWE-434', 268), ('CWE-287', 256), ('CWE-306', 227), ('CWE-476', 217), ('CWE-269', 215), ('CWE-798', 205), ('CWE-863', 197), ('CWE-190', 191), ('CWE-400', 183), ('CWE-502', 173), ('CWE-522', 162), ('CWE-200', 158), ('CWE-918', 155), ('CWE-77', 147), ('CWE-843', 147), ('CWE-732', 144), ('CWE-362', 139), ('CWE-295', 136), ('CWE-427', 123), ('CWE-611', 122), ('CWE-401', 122), ('CWE-74', 115), ('CWE-119', 111), ('CWE-319', 106), ('CWE-601', 105), ('CWE-129', 94), ('CWE-59', 94), ('CWE-94', 89), ('CWE-312', 86), ('CWE-327', 86), ('CWE-532', 79), ('CWE-203', 74), ('CWE-1321', 72), ('CWE-668', 71), ('CWE-917', 68), ('CWE-770', 66), ('CWE-617', 65), ('CWE-347', 65), ('CWE-908', 64), ('CWE-755', 60), ('CWE-613', 60), ('CWE-209', 58), ('CWE-835', 57), ('CWE-345', 55), ('CWE-307', 51), ('CWE-639', 48), ('CWE-415', 45), ('CWE-444', 44), ('CWE-330', 43), ('CWE-665', 42), ('CWE-754', 42), ('CWE-367', 41), ('CWE-1021', 40), ('CWE-1236', 38), ('CWE-494', 38), ('CWE-326', 37), ('CWE-426', 37), ('CWE-384', 36), ('CWE-674', 35), ('CWE-346', 34), ('CWE-369', 34), ('CWE-311', 33), ('CWE-404', 32), ('CWE-281', 31), ('CWE-290', 31), ('CWE-552', 30), ('CWE-922', 28), ('CWE-667', 27), ('CWE-88', 27), ('CWE-122', 27), ('CWE-1188', 26), ('CWE-191', 25), ('CWE-131', 25), ('CWE-116', 24), ('CWE-428', 23), ('CWE-521', 23), ('CWE-354', 22), ('CWE-294', 22), ('CWE-697', 19), ('CWE-824', 19), ('CWE-459', 18), ('CWE-909', 16), ('CWE-763', 16), ('CWE-610', 15), ('CWE-662', 14), ('CWE-425', 14), ('CWE-121', 14), ('CWE-134', 14), ('CWE-212', 14), ('CWE-829', 13), ('CWE-681', 13), ('CWE-640', 11), ('CWE-193', 11), ('CWE-916', 11), ('CWE-706', 11), ('CWE-670', 10), ('CWE-772', 10), ('CWE-436', 10), ('CWE-91', 9), ('CWE-669', 9), ('CWE-776', 8), ('CWE-682', 6), ('CWE-331', 6), ('CWE-252', 6), ('CWE-913', 6), ('CWE-288', 6), ('CWE-284', 6), ('CWE-672', 5), ('CWE-321', 5), ('CWE-266', 5), ('CWE-80', 4), ('CWE-377', 4), ('CWE-834', 4), ('CWE-822', 4), ('CWE-565', 4), ('CWE-338', 4), ('CWE-704', 3), ('CWE-178', 3), ('CWE-838', 3), ('CWE-273', 3), ('CWE-335', 3), ('CWE-749', 3), ('CWE-305', 3), ('CWE-250', 3), ('CWE-270', 2), ('CWE-201', 2), ('CWE-208', 2), ('CWE-1333', 2), ('CWE-285', 2), ('CWE-707', 2), ('CWE-349', 2), ('CWE-117', 2), ('CWE-61', 2), ('CWE-342', 2), ('CWE-924', 2), ('CWE-385', 2), ('CWE-912', 2), ('CWE-113', 2), ('CWE-805', 1), ('CWE-299', 1), ('CWE-1286', 1), ('CWE-114', 1), ('CWE-73', 1), ('CWE-506', 1), ('CWE-684', 1), ('CWE-184', 1), ('CWE-185', 1), ('CWE-757', 1), ('CWE-23', 1), ('CWE-435', 1), ('CWE-323', 1), ('CWE-248', 1), ('CWE-920', 1), ('CWE-126', 1), ('CWE-788', 1), ('CWE-195', 1), ('CWE-130', 1), ('CWE-364', 1), ('CWE-123', 1), ('CWE-271', 1), ('CWE-303', 1), ('CWE-693', 1), ('CWE-489', 1), ('CWE-279', 1), ('CWE-334', 1), ('CWE-297', 1), ('CWE-441', 1), ('CWE-358', 1), ('CWE-214', 1), ('CWE-760', 1), ('CWE-643', 1), ('CWE-507', 1), ('CWE-93', 1), ('CWE-115', 1), ('CWE-759', 1), ('CWE-603', 1), ('CWE-471', 1), ('CWE-1022', 1), ('CWE-350', 1), ('CWE-259', 1), ('CWE-567', 1), ('CWE-112', 1)]
195
{'CWE-120', 'CWE-787', 'CWE-20', 'CWE-22', 'CWE-352', 'CWE-78', 'CWE-416', 'CWE-125', 'NVD-CWE-Other', 'CWE-89', 'CWE-79'}
11
8100
11

<wv------------------------------------------------------------------------------------------->
year = '2020'
vec_len = 100
min_count = 1
window_len = 5
wv_model_path  = '..//..//models//wv//'+year+"_"+str(vec_len)+"_"+str(min_count)+"_"+str(window_len)+'.pkl'
label_path = '..\\..\\data\\clean\\nvdcve-1.1-'+year+'_labels.csv'
n=30
cwe_count = 11

self.c1 = Conv2D(filters=12, kernel_size=(3, vec_len), padding='same')  # 卷积层
        self.b1 = BatchNormalization()  # BN层
        self.a1 = Activation('relu')  # 激活层
        self.p1 = MaxPool2D(pool_size=(2, 2), strides=2, padding='same')  # 池化层
        self.d1 = Dropout(0.2)  # dropout层

        self.flatten = Flatten()
        self.f1 = Dense(128, activation='relu')
        self.d2 = Dropout(0.2)
        self.f2 = Dense(cwe_count, activation='softmax')

203/203 [==============================] - 2s 3ms/step - loss: 2.6922 - accuracy: 0.3557
Epoch 2/10
203/203 [==============================] - 1s 3ms/step - loss: 1.3199 - accuracy: 0.5212
Epoch 3/10
203/203 [==============================] - 1s 3ms/step - loss: 1.1164 - accuracy: 0.5950
Epoch 4/10
203/203 [==============================] - 1s 3ms/step - loss: 1.0069 - accuracy: 0.6308
Epoch 5/10
203/203 [==============================] - 1s 3ms/step - loss: 0.9305 - accuracy: 0.6479
Epoch 6/10
203/203 [==============================] - 1s 3ms/step - loss: 0.8358 - accuracy: 0.6806
Epoch 7/10
203/203 [==============================] - 1s 3ms/step - loss: 0.8195 - accuracy: 0.6895
Epoch 8/10
203/203 [==============================] - 1s 3ms/step - loss: 0.7475 - accuracy: 0.7164
Epoch 9/10
203/203 [==============================] - 1s 3ms/step - loss: 0.7371 - accuracy: 0.7214
Epoch 10/10
203/203 [==============================] - 1s 3ms/step - loss: 0.6770 - accuracy: 0.7446
51/51 [==============================] - 0s 2ms/step - loss: 0.7384 - accuracy: 0.7549
last score: [0.7383791208267212, 0.7549382448196411]
</wv------------------------------------------------------------------------------------------->

<tfi-wv---------------------------------------------------------------------------------------->
year = '2020'
vec_len = 100
min_count = 1
window_len = 5
ig_num = 1000
sentence_len = 30
n=sentence_len
cwe_count = 11  # 2020

03/203 [==============================] - 2s 3ms/step - loss: 2.7412 - accuracy: 0.3089
Epoch 2/10
203/203 [==============================] - 1s 3ms/step - loss: 1.3039 - accuracy: 0.5249
Epoch 3/10
203/203 [==============================] - 1s 3ms/step - loss: 1.1102 - accuracy: 0.5785
Epoch 4/10
203/203 [==============================] - 1s 3ms/step - loss: 1.0264 - accuracy: 0.6147
Epoch 5/10
203/203 [==============================] - 1s 3ms/step - loss: 0.9598 - accuracy: 0.6322
Epoch 6/10
203/203 [==============================] - 1s 3ms/step - loss: 0.9061 - accuracy: 0.6571
Epoch 7/10
203/203 [==============================] - 1s 3ms/step - loss: 0.8630 - accuracy: 0.6640
Epoch 8/10
203/203 [==============================] - 1s 3ms/step - loss: 0.8227 - accuracy: 0.6776
Epoch 9/10
203/203 [==============================] - 1s 3ms/step - loss: 0.8070 - accuracy: 0.6939
Epoch 10/10
203/203 [==============================] - 1s 3ms/step - loss: 0.7763 - accuracy: 0.7056
51/51 [==============================] - 0s 2ms/step - loss: 0.7199 - accuracy: 0.7506
last score: [0.7199015021324158, 0.750617265701294]
</tfi-wv----------------------------------------------------------------------------------------->

<fast------------------------------------------------------------------------------------------->
vec_len = 100
min_count = 1
window_len = 5
fast_model_path  = '..//..//models//fasttext//'+year+"_"+str(vec_len)+"_"+str(min_count)+"_"+str(window_len)+'.pkl'
label_path = '..\\..\\data\\clean\\nvdcve-1.1-'+year+'_labels.csv'
n=30
cwe_count = 11  # 2020

self.c1 = Conv2D(filters=12, kernel_size=(3, vec_len), padding='same')  # 卷积层
        self.b1 = BatchNormalization()  # BN层
        self.a1 = Activation('relu')  # 激活层
        self.p1 = MaxPool2D(pool_size=(2, 2), strides=2, padding='same')  # 池化层
        self.d1 = Dropout(0.2)  # dropout层

        self.flatten = Flatten()
        self.f1 = Dense(128, activation='relu')
        self.d2 = Dropout(0.2)
        self.f2 = Dense(cwe_count, activation='softmax')

203/203 [==============================] - 2s 3ms/step - loss: 2.9272 - accuracy: 0.3182
Epoch 2/10
203/203 [==============================] - 1s 3ms/step - loss: 1.3688 - accuracy: 0.4969
Epoch 3/10
203/203 [==============================] - 1s 3ms/step - loss: 1.1377 - accuracy: 0.5849
Epoch 4/10
203/203 [==============================] - 1s 3ms/step - loss: 1.0579 - accuracy: 0.6122
Epoch 5/10
203/203 [==============================] - 1s 3ms/step - loss: 0.9813 - accuracy: 0.6223
Epoch 6/10
203/203 [==============================] - 1s 3ms/step - loss: 0.9361 - accuracy: 0.6500
Epoch 7/10
203/203 [==============================] - 1s 3ms/step - loss: 0.8548 - accuracy: 0.6758
Epoch 8/10
203/203 [==============================] - 1s 3ms/step - loss: 0.8380 - accuracy: 0.6798
Epoch 9/10
203/203 [==============================] - 1s 3ms/step - loss: 0.8345 - accuracy: 0.6823
Epoch 10/10
203/203 [==============================] - 1s 3ms/step - loss: 0.7988 - accuracy: 0.6906
51/51 [==============================] - 0s 2ms/step - loss: 0.7751 - accuracy: 0.7302
last score: [0.7750535011291504, 0.730246901512146]

</fast------------------------------------------------------------------------------------------->


